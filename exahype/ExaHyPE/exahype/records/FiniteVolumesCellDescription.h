#ifndef _EXAHYPE_RECORDS_FINITEVOLUMESCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_FINITEVOLUMESCELLDESCRIPTION_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class FiniteVolumesCellDescription;
      class FiniteVolumesCellDescriptionPacked;
   }
}

#if defined(Parallel)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   04/01/2017 17:03
    */
   class exahype::records::FiniteVolumesCellDescription { 
      
      public:
         
         typedef exahype::records::FiniteVolumesCellDescriptionPacked Packed;
         
         enum RefinementEvent {
            None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
         };
         
         enum Type {
            Erased = 0, Ancestor = 1, EmptyAncestor = 2, Cell = 3, Descendant = 4, EmptyDescendant = 5
         };
         
         struct PersistentRecords {
            int _solverNumber;
            double _timeStepSize;
            double _timeStamp;
            double _previousTimeStepSize;
            int _solution;
            int _previousSolution;
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _offset;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _size;
            #endif
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
            #endif
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
            #endif
            bool _oneRemoteBoundaryNeighbourIsOfTypeCell;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter;
            #endif
            Type _type;
            int _parentIndex;
            RefinementEvent _refinementEvent;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solverNumber = solverNumber;
            }
            
            
            
            inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _timeStepSize;
            }
            
            
            
            inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _timeStepSize = timeStepSize;
            }
            
            
            
            inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _timeStamp;
            }
            
            
            
            inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _timeStamp = timeStamp;
            }
            
            
            
            inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousTimeStepSize;
            }
            
            
            
            inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousTimeStepSize = previousTimeStepSize;
            }
            
            
            
            inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solution;
            }
            
            
            
            inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solution = solution;
            }
            
            
            
            inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _previousSolution;
            }
            
            
            
            inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _previousSolution = previousSolution;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _offset = (offset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _size = (size);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _riemannSolvePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _riemannSolvePerformed = (riemannSolvePerformed);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInside;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInside = (isInside);
            }
            
            
            
            inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _faceDataExchangeCounter;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _faceDataExchangeCounter = (faceDataExchangeCounter);
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _type = type;
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentIndex = parentIndex;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementEvent = refinementEvent;
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         FiniteVolumesCellDescription();
         
         /**
          * Generated
          */
         FiniteVolumesCellDescription(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         FiniteVolumesCellDescription(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
         
         /**
          * Generated
          */
         ~FiniteVolumesCellDescription();
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solverNumber = solverNumber;
         }
         
         
         
         inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStepSize;
         }
         
         
         
         inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStepSize = timeStepSize;
         }
         
         
         
         inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._timeStamp;
         }
         
         
         
         inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._timeStamp = timeStamp;
         }
         
         
         
         inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousTimeStepSize;
         }
         
         
         
         inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousTimeStepSize = previousTimeStepSize;
         }
         
         
         
         inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solution;
         }
         
         
         
         inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solution = solution;
         }
         
         
         
         inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._previousSolution;
         }
         
         
         
         inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._previousSolution = previousSolution;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._offset = (offset);
         }
         
         
         
         inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._offset[elementIndex];
            
         }
         
         
         
         inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._offset[elementIndex]= offset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._size = (size);
         }
         
         
         
         inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._size[elementIndex];
            
         }
         
         
         
         inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._size[elementIndex]= size;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._riemannSolvePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
         }
         
         
         
         inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._riemannSolvePerformed[elementIndex];
            
         }
         
         
         
         inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
            
         }
         
         
         
         inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._riemannSolvePerformed.flip(elementIndex);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isInside;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isInside = (isInside);
         }
         
         
         
         inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._isInside[elementIndex];
            
         }
         
         
         
         inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._isInside[elementIndex]= isInside;
            
         }
         
         
         
         inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._isInside.flip(elementIndex);
         }
         
         
         
         inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell;
         }
         
         
         
         inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._faceDataExchangeCounter;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._faceDataExchangeCounter = (faceDataExchangeCounter);
         }
         
         
         
         inline int getFaceDataExchangeCounter(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._faceDataExchangeCounter[elementIndex];
            
         }
         
         
         
         inline void setFaceDataExchangeCounter(int elementIndex, const int& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._faceDataExchangeCounter[elementIndex]= faceDataExchangeCounter;
            
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._type = type;
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentIndex = parentIndex;
         }
         
         
         
         inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementEvent;
         }
         
         
         
         inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementEvent = refinementEvent;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const RefinementEvent& param);
         
         /**
          * Generated
          */
         static std::string getRefinementEventMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Type& param);
         
         /**
          * Generated
          */
         static std::string getTypeMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         FiniteVolumesCellDescriptionPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            /**
             * @param communicateSleep -1 Data exchange through blocking mpi
             * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
             * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
             */
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 09-02-2014 14:40
             *
             * @date   04/01/2017 17:03
             */
            class exahype::records::FiniteVolumesCellDescriptionPacked { 
               
               public:
                  
                  typedef exahype::records::FiniteVolumesCellDescription::Type Type;
                  
                  typedef exahype::records::FiniteVolumesCellDescription::RefinementEvent RefinementEvent;
                  
                  struct PersistentRecords {
                     int _solverNumber;
                     double _timeStepSize;
                     double _timeStamp;
                     double _previousTimeStepSize;
                     int _solution;
                     int _previousSolution;
                     int _level;
                     tarch::la::Vector<DIMENSIONS,double> _offset;
                     tarch::la::Vector<DIMENSIONS,double> _size;
                     std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
                     std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
                     bool _oneRemoteBoundaryNeighbourIsOfTypeCell;
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter;
                     Type _type;
                     int _parentIndex;
                     RefinementEvent _refinementEvent;
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
                     
                     
                     inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solverNumber;
                     }
                     
                     
                     
                     inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solverNumber = solverNumber;
                     }
                     
                     
                     
                     inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _timeStepSize;
                     }
                     
                     
                     
                     inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _timeStepSize = timeStepSize;
                     }
                     
                     
                     
                     inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _timeStamp;
                     }
                     
                     
                     
                     inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _timeStamp = timeStamp;
                     }
                     
                     
                     
                     inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _previousTimeStepSize;
                     }
                     
                     
                     
                     inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _previousTimeStepSize = previousTimeStepSize;
                     }
                     
                     
                     
                     inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solution;
                     }
                     
                     
                     
                     inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solution = solution;
                     }
                     
                     
                     
                     inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _previousSolution;
                     }
                     
                     
                     
                     inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _previousSolution = previousSolution;
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _level = level;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _offset;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _offset = (offset);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _size;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _size = (size);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _riemannSolvePerformed;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _riemannSolvePerformed = (riemannSolvePerformed);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isInside;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isInside = (isInside);
                     }
                     
                     
                     
                     inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _oneRemoteBoundaryNeighbourIsOfTypeCell;
                     }
                     
                     
                     
                     inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _faceDataExchangeCounter;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _faceDataExchangeCounter = (faceDataExchangeCounter);
                     }
                     
                     
                     
                     inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _type;
                     }
                     
                     
                     
                     inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _type = type;
                     }
                     
                     
                     
                     inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _parentIndex;
                     }
                     
                     
                     
                     inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _parentIndex = parentIndex;
                     }
                     
                     
                     
                     inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _refinementEvent;
                     }
                     
                     
                     
                     inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _refinementEvent = refinementEvent;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  
               public:
                  /**
                   * Generated
                   */
                  FiniteVolumesCellDescriptionPacked();
                  
                  /**
                   * Generated
                   */
                  FiniteVolumesCellDescriptionPacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  FiniteVolumesCellDescriptionPacked(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& oneRemoteBoundaryNeighbourIsOfTypeCell, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
                  
                  /**
                   * Generated
                   */
                  ~FiniteVolumesCellDescriptionPacked();
                  
                  
                  inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solverNumber;
                  }
                  
                  
                  
                  inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solverNumber = solverNumber;
                  }
                  
                  
                  
                  inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._timeStepSize;
                  }
                  
                  
                  
                  inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._timeStepSize = timeStepSize;
                  }
                  
                  
                  
                  inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._timeStamp;
                  }
                  
                  
                  
                  inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._timeStamp = timeStamp;
                  }
                  
                  
                  
                  inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._previousTimeStepSize;
                  }
                  
                  
                  
                  inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._previousTimeStepSize = previousTimeStepSize;
                  }
                  
                  
                  
                  inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solution;
                  }
                  
                  
                  
                  inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solution = solution;
                  }
                  
                  
                  
                  inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._previousSolution;
                  }
                  
                  
                  
                  inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._previousSolution = previousSolution;
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._offset;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._offset = (offset);
                  }
                  
                  
                  
                  inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._offset[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._offset[elementIndex]= offset;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._size;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._size = (size);
                  }
                  
                  
                  
                  inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._size[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._size[elementIndex]= size;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._riemannSolvePerformed;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
                  }
                  
                  
                  
                  inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._riemannSolvePerformed[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
                     
                  }
                  
                  
                  
                  inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._riemannSolvePerformed.flip(elementIndex);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isInside;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isInside = (isInside);
                  }
                  
                  
                  
                  inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._isInside[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._isInside[elementIndex]= isInside;
                     
                  }
                  
                  
                  
                  inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._isInside.flip(elementIndex);
                  }
                  
                  
                  
                  inline bool getOneRemoteBoundaryNeighbourIsOfTypeCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell;
                  }
                  
                  
                  
                  inline void setOneRemoteBoundaryNeighbourIsOfTypeCell(const bool& oneRemoteBoundaryNeighbourIsOfTypeCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._oneRemoteBoundaryNeighbourIsOfTypeCell = oneRemoteBoundaryNeighbourIsOfTypeCell;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._faceDataExchangeCounter;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._faceDataExchangeCounter = (faceDataExchangeCounter);
                  }
                  
                  
                  
                  inline int getFaceDataExchangeCounter(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._faceDataExchangeCounter[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setFaceDataExchangeCounter(int elementIndex, const int& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._faceDataExchangeCounter[elementIndex]= faceDataExchangeCounter;
                     
                  }
                  
                  
                  
                  inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._type;
                  }
                  
                  
                  
                  inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._type = type;
                  }
                  
                  
                  
                  inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._parentIndex;
                  }
                  
                  
                  
                  inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._parentIndex = parentIndex;
                  }
                  
                  
                  
                  inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._refinementEvent;
                  }
                  
                  
                  
                  inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._refinementEvent = refinementEvent;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const Type& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getTypeMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const RefinementEvent& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getRefinementEventMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  FiniteVolumesCellDescription convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     /**
                      * @param communicateSleep -1 Data exchange through blocking mpi
                      * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                      * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                      */
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     #endif
                        
                     };
                     
                     #ifdef PackedRecords
                     #pragma pack (pop)
                     #endif
                     
                     
                     
                  #elif !defined(Parallel)
                     /**
                      * @author This class is generated by DaStGen
                      * 		   DataStructureGenerator (DaStGen)
                      * 		   2007-2009 Wolfgang Eckhardt
                      * 		   2012      Tobias Weinzierl
                      *
                      * 		   build date: 09-02-2014 14:40
                      *
                      * @date   04/01/2017 17:03
                      */
                     class exahype::records::FiniteVolumesCellDescription { 
                        
                        public:
                           
                           typedef exahype::records::FiniteVolumesCellDescriptionPacked Packed;
                           
                           enum RefinementEvent {
                              None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
                           };
                           
                           enum Type {
                              Erased = 0, Ancestor = 1, EmptyAncestor = 2, Cell = 3, Descendant = 4, EmptyDescendant = 5
                           };
                           
                           struct PersistentRecords {
                              int _solverNumber;
                              double _timeStepSize;
                              double _timeStamp;
                              double _previousTimeStepSize;
                              int _solution;
                              int _previousSolution;
                              int _level;
                              #ifdef UseManualAlignment
                              tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
                              #else
                              tarch::la::Vector<DIMENSIONS,double> _offset;
                              #endif
                              #ifdef UseManualAlignment
                              tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
                              #else
                              tarch::la::Vector<DIMENSIONS,double> _size;
                              #endif
                              #ifdef UseManualAlignment
                              std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed __attribute__((aligned(VectorisationAlignment)));
                              #else
                              std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
                              #endif
                              #ifdef UseManualAlignment
                              std::bitset<DIMENSIONS_TIMES_TWO> _isInside __attribute__((aligned(VectorisationAlignment)));
                              #else
                              std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
                              #endif
                              Type _type;
                              int _parentIndex;
                              RefinementEvent _refinementEvent;
                              /**
                               * Generated
                               */
                              PersistentRecords();
                              
                              /**
                               * Generated
                               */
                              PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
                              
                              
                              inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solverNumber;
                              }
                              
                              
                              
                              inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solverNumber = solverNumber;
                              }
                              
                              
                              
                              inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _timeStepSize;
                              }
                              
                              
                              
                              inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _timeStepSize = timeStepSize;
                              }
                              
                              
                              
                              inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _timeStamp;
                              }
                              
                              
                              
                              inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _timeStamp = timeStamp;
                              }
                              
                              
                              
                              inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _previousTimeStepSize;
                              }
                              
                              
                              
                              inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _previousTimeStepSize = previousTimeStepSize;
                              }
                              
                              
                              
                              inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solution;
                              }
                              
                              
                              
                              inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solution = solution;
                              }
                              
                              
                              
                              inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _previousSolution;
                              }
                              
                              
                              
                              inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _previousSolution = previousSolution;
                              }
                              
                              
                              
                              inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _level;
                              }
                              
                              
                              
                              inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _level = level;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _offset;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _offset = (offset);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _size;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _size = (size);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _riemannSolvePerformed;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _riemannSolvePerformed = (riemannSolvePerformed);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _isInside;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _isInside = (isInside);
                              }
                              
                              
                              
                              inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _type;
                              }
                              
                              
                              
                              inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _type = type;
                              }
                              
                              
                              
                              inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _parentIndex;
                              }
                              
                              
                              
                              inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _parentIndex = parentIndex;
                              }
                              
                              
                              
                              inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _refinementEvent;
                              }
                              
                              
                              
                              inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _refinementEvent = refinementEvent;
                              }
                              
                              
                              
                           };
                           
                        private: 
                           PersistentRecords _persistentRecords;
                           
                        public:
                           /**
                            * Generated
                            */
                           FiniteVolumesCellDescription();
                           
                           /**
                            * Generated
                            */
                           FiniteVolumesCellDescription(const PersistentRecords& persistentRecords);
                           
                           /**
                            * Generated
                            */
                           FiniteVolumesCellDescription(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
                           
                           /**
                            * Generated
                            */
                           ~FiniteVolumesCellDescription();
                           
                           
                           inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solverNumber;
                           }
                           
                           
                           
                           inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solverNumber = solverNumber;
                           }
                           
                           
                           
                           inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._timeStepSize;
                           }
                           
                           
                           
                           inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._timeStepSize = timeStepSize;
                           }
                           
                           
                           
                           inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._timeStamp;
                           }
                           
                           
                           
                           inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._timeStamp = timeStamp;
                           }
                           
                           
                           
                           inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._previousTimeStepSize;
                           }
                           
                           
                           
                           inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._previousTimeStepSize = previousTimeStepSize;
                           }
                           
                           
                           
                           inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solution;
                           }
                           
                           
                           
                           inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solution = solution;
                           }
                           
                           
                           
                           inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._previousSolution;
                           }
                           
                           
                           
                           inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._previousSolution = previousSolution;
                           }
                           
                           
                           
                           inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._level;
                           }
                           
                           
                           
                           inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._level = level;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._offset;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._offset = (offset);
                           }
                           
                           
                           
                           inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              return _persistentRecords._offset[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._offset[elementIndex]= offset;
                              
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._size;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._size = (size);
                           }
                           
                           
                           
                           inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              return _persistentRecords._size[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._size[elementIndex]= size;
                              
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._riemannSolvePerformed;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
                           }
                           
                           
                           
                           inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._riemannSolvePerformed[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
                              
                           }
                           
                           
                           
                           inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._riemannSolvePerformed.flip(elementIndex);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._isInside;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._isInside = (isInside);
                           }
                           
                           
                           
                           inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._isInside[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._isInside[elementIndex]= isInside;
                              
                           }
                           
                           
                           
                           inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._isInside.flip(elementIndex);
                           }
                           
                           
                           
                           inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._type;
                           }
                           
                           
                           
                           inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._type = type;
                           }
                           
                           
                           
                           inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._parentIndex;
                           }
                           
                           
                           
                           inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._parentIndex = parentIndex;
                           }
                           
                           
                           
                           inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._refinementEvent;
                           }
                           
                           
                           
                           inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._refinementEvent = refinementEvent;
                           }
                           
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const RefinementEvent& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getRefinementEventMapping();
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const Type& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getTypeMapping();
                           
                           /**
                            * Generated
                            */
                           std::string toString() const;
                           
                           /**
                            * Generated
                            */
                           void toString(std::ostream& out) const;
                           
                           
                           PersistentRecords getPersistentRecords() const;
                           /**
                            * Generated
                            */
                           FiniteVolumesCellDescriptionPacked convert() const;
                           
                           
                        #ifdef Parallel
                           protected:
                              static tarch::logging::Log _log;
                              
                           public:
                              
                              /**
                               * Global that represents the mpi datatype.
                               * There are two variants: Datatype identifies only those attributes marked with
                               * parallelise. FullDatatype instead identifies the whole record with all fields.
                               */
                              static MPI_Datatype Datatype;
                              static MPI_Datatype FullDatatype;
                              
                              /**
                               * Initializes the data type for the mpi operations. Has to be called
                               * before the very first send or receive operation is called.
                               */
                              static void initDatatype();
                              
                              static void shutdownDatatype();
                              
                              /**
                               * @param communicateSleep -1 Data exchange through blocking mpi
                               * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                               * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                               */
                              void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              #endif
                                 
                              };
                              
                              #ifndef DaStGenPackedPadding
                                #define DaStGenPackedPadding 1      // 32 bit version
                                // #define DaStGenPackedPadding 2   // 64 bit version
                              #endif
                              
                              
                              #ifdef PackedRecords
                                 #pragma pack (push, DaStGenPackedPadding)
                              #endif
                              
                              /**
                               * @author This class is generated by DaStGen
                               * 		   DataStructureGenerator (DaStGen)
                               * 		   2007-2009 Wolfgang Eckhardt
                               * 		   2012      Tobias Weinzierl
                               *
                               * 		   build date: 09-02-2014 14:40
                               *
                               * @date   04/01/2017 17:03
                               */
                              class exahype::records::FiniteVolumesCellDescriptionPacked { 
                                 
                                 public:
                                    
                                    typedef exahype::records::FiniteVolumesCellDescription::Type Type;
                                    
                                    typedef exahype::records::FiniteVolumesCellDescription::RefinementEvent RefinementEvent;
                                    
                                    struct PersistentRecords {
                                       int _solverNumber;
                                       double _timeStepSize;
                                       double _timeStamp;
                                       double _previousTimeStepSize;
                                       int _solution;
                                       int _previousSolution;
                                       int _level;
                                       tarch::la::Vector<DIMENSIONS,double> _offset;
                                       tarch::la::Vector<DIMENSIONS,double> _size;
                                       std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
                                       std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
                                       Type _type;
                                       int _parentIndex;
                                       RefinementEvent _refinementEvent;
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords();
                                       
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
                                       
                                       
                                       inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solverNumber;
                                       }
                                       
                                       
                                       
                                       inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solverNumber = solverNumber;
                                       }
                                       
                                       
                                       
                                       inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _timeStepSize;
                                       }
                                       
                                       
                                       
                                       inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _timeStepSize = timeStepSize;
                                       }
                                       
                                       
                                       
                                       inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _timeStamp;
                                       }
                                       
                                       
                                       
                                       inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _timeStamp = timeStamp;
                                       }
                                       
                                       
                                       
                                       inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _previousTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _previousTimeStepSize = previousTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solution;
                                       }
                                       
                                       
                                       
                                       inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solution = solution;
                                       }
                                       
                                       
                                       
                                       inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _previousSolution;
                                       }
                                       
                                       
                                       
                                       inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _previousSolution = previousSolution;
                                       }
                                       
                                       
                                       
                                       inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _level;
                                       }
                                       
                                       
                                       
                                       inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _level = level;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _offset;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _offset = (offset);
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _size;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _size = (size);
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _riemannSolvePerformed;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _riemannSolvePerformed = (riemannSolvePerformed);
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _isInside;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _isInside = (isInside);
                                       }
                                       
                                       
                                       
                                       inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _type;
                                       }
                                       
                                       
                                       
                                       inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _type = type;
                                       }
                                       
                                       
                                       
                                       inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _parentIndex;
                                       }
                                       
                                       
                                       
                                       inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _parentIndex = parentIndex;
                                       }
                                       
                                       
                                       
                                       inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _refinementEvent;
                                       }
                                       
                                       
                                       
                                       inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _refinementEvent = refinementEvent;
                                       }
                                       
                                       
                                       
                                    };
                                    
                                 private: 
                                    PersistentRecords _persistentRecords;
                                    
                                 public:
                                    /**
                                     * Generated
                                     */
                                    FiniteVolumesCellDescriptionPacked();
                                    
                                    /**
                                     * Generated
                                     */
                                    FiniteVolumesCellDescriptionPacked(const PersistentRecords& persistentRecords);
                                    
                                    /**
                                     * Generated
                                     */
                                    FiniteVolumesCellDescriptionPacked(const int& solverNumber, const double& timeStepSize, const double& timeStamp, const double& previousTimeStepSize, const int& solution, const int& previousSolution, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const Type& type, const int& parentIndex, const RefinementEvent& refinementEvent);
                                    
                                    /**
                                     * Generated
                                     */
                                    ~FiniteVolumesCellDescriptionPacked();
                                    
                                    
                                    inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solverNumber;
                                    }
                                    
                                    
                                    
                                    inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solverNumber = solverNumber;
                                    }
                                    
                                    
                                    
                                    inline double getTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._timeStepSize;
                                    }
                                    
                                    
                                    
                                    inline void setTimeStepSize(const double& timeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._timeStepSize = timeStepSize;
                                    }
                                    
                                    
                                    
                                    inline double getTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._timeStamp;
                                    }
                                    
                                    
                                    
                                    inline void setTimeStamp(const double& timeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._timeStamp = timeStamp;
                                    }
                                    
                                    
                                    
                                    inline double getPreviousTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._previousTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline void setPreviousTimeStepSize(const double& previousTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._previousTimeStepSize = previousTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solution;
                                    }
                                    
                                    
                                    
                                    inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solution = solution;
                                    }
                                    
                                    
                                    
                                    inline int getPreviousSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._previousSolution;
                                    }
                                    
                                    
                                    
                                    inline void setPreviousSolution(const int& previousSolution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._previousSolution = previousSolution;
                                    }
                                    
                                    
                                    
                                    inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._level;
                                    }
                                    
                                    
                                    
                                    inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._level = level;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._offset;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._offset = (offset);
                                    }
                                    
                                    
                                    
                                    inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       return _persistentRecords._offset[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       _persistentRecords._offset[elementIndex]= offset;
                                       
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._size;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._size = (size);
                                    }
                                    
                                    
                                    
                                    inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       return _persistentRecords._size[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       _persistentRecords._size[elementIndex]= size;
                                       
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._riemannSolvePerformed;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
                                    }
                                    
                                    
                                    
                                    inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       return _persistentRecords._riemannSolvePerformed[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
                                       
                                    }
                                    
                                    
                                    
                                    inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._riemannSolvePerformed.flip(elementIndex);
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._isInside;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._isInside = (isInside);
                                    }
                                    
                                    
                                    
                                    inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       return _persistentRecords._isInside[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._isInside[elementIndex]= isInside;
                                       
                                    }
                                    
                                    
                                    
                                    inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._isInside.flip(elementIndex);
                                    }
                                    
                                    
                                    
                                    inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._type;
                                    }
                                    
                                    
                                    
                                    inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._type = type;
                                    }
                                    
                                    
                                    
                                    inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._parentIndex;
                                    }
                                    
                                    
                                    
                                    inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._parentIndex = parentIndex;
                                    }
                                    
                                    
                                    
                                    inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._refinementEvent;
                                    }
                                    
                                    
                                    
                                    inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._refinementEvent = refinementEvent;
                                    }
                                    
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const Type& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getTypeMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const RefinementEvent& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getRefinementEventMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    std::string toString() const;
                                    
                                    /**
                                     * Generated
                                     */
                                    void toString(std::ostream& out) const;
                                    
                                    
                                    PersistentRecords getPersistentRecords() const;
                                    /**
                                     * Generated
                                     */
                                    FiniteVolumesCellDescription convert() const;
                                    
                                    
                                 #ifdef Parallel
                                    protected:
                                       static tarch::logging::Log _log;
                                       
                                    public:
                                       
                                       /**
                                        * Global that represents the mpi datatype.
                                        * There are two variants: Datatype identifies only those attributes marked with
                                        * parallelise. FullDatatype instead identifies the whole record with all fields.
                                        */
                                       static MPI_Datatype Datatype;
                                       static MPI_Datatype FullDatatype;
                                       
                                       /**
                                        * Initializes the data type for the mpi operations. Has to be called
                                        * before the very first send or receive operation is called.
                                        */
                                       static void initDatatype();
                                       
                                       static void shutdownDatatype();
                                       
                                       /**
                                        * @param communicateSleep -1 Data exchange through blocking mpi
                                        * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                        * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                        */
                                       void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                       
                                       void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                       
                                       static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                       
                                       #endif
                                          
                                       };
                                       
                                       #ifdef PackedRecords
                                       #pragma pack (pop)
                                       #endif
                                       
                                       
                                       
                                    
                                 #endif
                                 
                                 #endif
                                 
